import { afterAll, beforeAll, describe, expect, it, vi } from 'vitest';
import { UserConfig } from '../config';
import { ChatService } from '../features/chat/chat.service';
import { prisma } from '../lib/Prisma';
import { logger } from '../utils/logger';

// Mock RabbitMQ
const mockMessageQueue = {
    sendToQueue: vi.fn(),
    isHealthy: vi.fn().mockReturnValue(true)
};

describe('ChatService Integration with getCombinedRelevantContext', () => {
    const testUserId = 'test-user-integration';
    const testConversationId = 'test-conversation-integration';
    let chatService: ChatService;

    beforeAll(async () => {
        // Setup mock UserConfig
        Object.assign(UserConfig, {
            id: testUserId,
            conversationId: testConversationId
        });

        // Create chat service instance
        chatService = new ChatService(mockMessageQueue as any);

        // Create test user and conversation
        try {
            await prisma.user.upsert({
                where: { id: testUserId },
                update: {},
                create: {
                    id: testUserId,
                    email: 'test-integration@example.com',
                    name: 'Test User Integration',
                    isActive: true
                }
            });

            await prisma.conversation.upsert({
                where: { id: testConversationId },
                update: {},
                create: {
                    id: testConversationId,
                    userId: testUserId,
                    title: 'Test Conversation for Integration'
                }
            });

            logger.info('âœ… Integration test setup completed');
        } catch (error) {
            logger.error('Error setting up integration test:', error);
        }
    });

    afterAll(async () => {
        // Clean up test data
        try {
            await prisma.message.deleteMany({
                where: { conversationId: testConversationId }
            });
            await prisma.conversation.deleteMany({
                where: { id: testConversationId }
            });
            await prisma.user.deleteMany({
                where: { id: testUserId }
            });
            logger.info('âœ… Integration test cleanup completed');
        } catch (error) {
            logger.error('Error cleaning up integration test:', error);
        }
    });

    it('should use getCombinedRelevantContext in addMessage flow', async () => {
        // Mock LLM call to avoid actual API calls
        const mockLLMResponse = 'This is a test response from the AI assistant.';
        
        // Create a spy to verify getCombinedRelevantContext is called
        const { getCombinedRelevantContext } = await import('../features/chat/chat.service.js');
        const getCombinedRelevantContextSpy = vi.spyOn(
            { getCombinedRelevantContext }, 
            'getCombinedRelevantContext'
        ).mockResolvedValue({
            messages: [
                {
                    id: 'test-msg-1',
                    role: 'user',
                    content: 'Hello, how are you?',
                    timestamp: Date.now() - 10000,
                    source: 'database',
                    conversationId: testConversationId,
                    metadata: {}
                },
                {
                    id: 'test-msg-2',
                    role: 'assistant',
                    content: 'I am doing well, thank you!',
                    timestamp: Date.now() - 5000,
                    source: 'vector',
                    conversationId: testConversationId,
                    metadata: {}
                }
            ],
            totalMessages: 2,
            vectorMessages: 1,
            databaseMessages: 1,
            formattedContext: 'ğŸ“[user] (20/8/2025 01:50): Hello, how are you?\nğŸ”[assistant] (20/8/2025 01:50): I am doing well, thank you!'
        });

        // Mock the LLM call - create a simple mock function
        const mockLLM = {
            invoke: vi.fn().mockResolvedValue({
                content: mockLLMResponse
            })
        };

        // Mock the module
        vi.doMock('../lib/LLMClient.js', () => ({
            llm: mockLLM
        }));

        const testMessage = 'What can you tell me about artificial intelligence?';

        try {
            // This should call getCombinedRelevantContext internally
            await chatService.addMessage(testMessage);

            // Verify that getCombinedRelevantContext was called with correct parameters
            expect(getCombinedRelevantContextSpy).toHaveBeenCalledWith(
                testMessage,
                testUserId,
                testConversationId,
                12 // The limit we set in the implementation
            );

            logger.info('âœ… getCombinedRelevantContext integration test passed');
        } catch (error) {
            // This is expected since we're mocking the LLM and there might be other dependencies
            logger.info('Expected error due to mocked dependencies:', error instanceof Error ? error.message : 'Unknown error');
            
            // Still verify the function was called
            expect(getCombinedRelevantContextSpy).toHaveBeenCalled();
        }

        // Restore the spy
        getCombinedRelevantContextSpy.mockRestore();
    }, 30000);

    it('should handle the new context format correctly', async () => {
        // Test that the new context format is processed correctly
        const mockCombinedContext = {
            messages: [
                {
                    id: 'db-msg-1',
                    role: 'user',
                    content: 'Test database message',
                    timestamp: Date.now() - 20000,
                    source: 'database' as const,
                    conversationId: testConversationId,
                    metadata: {}
                },
                {
                    id: 'vector-msg-1',
                    role: 'assistant',
                    content: 'Test vector message',
                    timestamp: Date.now() - 15000,
                    source: 'vector' as const,
                    conversationId: testConversationId,
                    metadata: {}
                }
            ],
            totalMessages: 2,
            vectorMessages: 1,
            databaseMessages: 1,
            formattedContext: 'ğŸ“[user] (20/8/2025 01:45): Test database message\nğŸ”[assistant] (20/8/2025 01:45): Test vector message'
        };

        // Test the context processing logic
        const databaseMessages = mockCombinedContext.messages
            .filter(msg => msg.source === 'database')
            .map(msg => {
                const dateObj = new Date(msg.timestamp);
                const formattedDate = dateObj.toLocaleDateString('id-ID', {
                    year: 'numeric',
                    month: 'numeric',
                    day: 'numeric',
                });
                const formattedTime = dateObj.toLocaleTimeString('id-ID', {
                    hour: '2-digit',
                    minute: '2-digit',
                    hour12: false,
                });
                const cleanContent = msg.content.replace(/\s+/g, ' ').trim();
                return `[${msg.role}] (${formattedDate} ${formattedTime}): ${cleanContent}`;
            });

        expect(databaseMessages).toHaveLength(1);
        expect(databaseMessages[0]).toContain('Test database message');
        expect(mockCombinedContext.formattedContext).toContain('ğŸ“[user]');
        expect(mockCombinedContext.formattedContext).toContain('ğŸ”[assistant]');

        logger.info('âœ… Context format processing test passed');
    });
});
